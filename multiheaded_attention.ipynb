{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a0817f5-7a9a-4c47-920b-4224d5342cf6",
   "metadata": {},
   "source": [
    "## Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0096daf-3c61-45d8-993f-b79a88e43974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0391,  0.0267, -0.0697]]], grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the input tensor\n",
    "x = torch.tensor([[[-0.1, 0.1,  0.3]]])\n",
    "\n",
    "# Create the multi-head attention layer\n",
    "layer = nn.MultiheadAttention(embed_dim=3, num_heads=1, bias=False, batch_first=True)\n",
    "\n",
    "custom_weights = torch.tensor( [[-0.3561,  0.3674, -0.5108],\n",
    "                                [ 0.5146, -0.4764, -0.1490],\n",
    "                                [ 0.5072, -0.2932, -0.5633],\n",
    "                                [-0.4932, -0.4468,  0.0736],\n",
    "                                [-0.6879, -0.4689, -0.1026],\n",
    "                                [ 0.1847,  0.1858,  0.4469],\n",
    "                                [-0.4110, -0.4083, -0.5549],\n",
    "                                [ 0.3921, -0.0746, -0.1336],\n",
    "                                [-0.6555, -0.3418, -0.2980]]).float()\n",
    "layer.in_proj_weight = nn.Parameter(custom_weights)\n",
    "\n",
    "custom_out_proj = torch.tensor([[-0.3601,  0.2771, -0.0573],\n",
    "                                [-0.0896,  0.0567, -0.2882],\n",
    "                                [ 0.3200,  0.1517,  0.0580]]).float()\n",
    "layer.out_proj.weight = nn.Parameter(custom_out_proj)\n",
    "\n",
    "# Perform the forward pass\n",
    "# You can use x for both queries, keys, and values in this example\n",
    "output_tensor, attn_output_weights = layer(x, x, x)  \n",
    "\n",
    "# Print the shape of the output tensor\n",
    "print(output_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8085ff43-a244-4930-9fea-d9081542c52e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d373581-b631-479b-8a6a-45c35c72a303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0391,  0.0267, -0.0697]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the input tensor\n",
    "x = torch.tensor([[[-0.1, 0.1, 0.3]]])\n",
    "\n",
    "q = torch.tensor(  [[-0.3561,  0.3674, -0.5108],\n",
    "                    [ 0.5146, -0.4764, -0.1490],\n",
    "                    [ 0.5072, -0.2932, -0.5633]]).float()\n",
    "k = torch.tensor(  [[-0.4932, -0.4468,  0.0736],\n",
    "                    [-0.6879, -0.4689, -0.1026],\n",
    "                    [ 0.1847,  0.1858,  0.4469]]).float()\n",
    "v = torch.tensor(  [[-0.4110, -0.4083, -0.5549],\n",
    "                    [ 0.3921, -0.0746, -0.1336],\n",
    "                    [-0.6555, -0.3418, -0.2980]]).float()\n",
    "o = torch.tensor([[-0.3601,  0.2771, -0.0573],\n",
    "                  [-0.0896,  0.0567, -0.2882],\n",
    "                  [ 0.3200,  0.1517,  0.0580]]).float()\n",
    "\n",
    "# Define the model parameters\n",
    "embed_dim = 3\n",
    "num_heads = 1\n",
    "head_dim = embed_dim // num_heads\n",
    "\n",
    "# Step 1: Linear projections for queries, keys, and values\n",
    "query_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "key_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "value_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "\n",
    "# Custom weights for linear projections\n",
    "query_proj.weight = nn.Parameter(q)\n",
    "key_proj.weight = nn.Parameter(k)\n",
    "value_proj.weight = nn.Parameter(v)\n",
    "\n",
    "# Step 2: Split the input into multiple heads\n",
    "query = query_proj(x)\n",
    "key = key_proj(x)\n",
    "value = value_proj(x)\n",
    "\n",
    "# Reshape query, key, and value to have shape (batch_size, num_heads, seq_len, head_dim)\n",
    "query = query.view(1, num_heads, -1, head_dim)\n",
    "key = key.view(1, num_heads, -1, head_dim)\n",
    "value = value.view(1, num_heads, -1, head_dim)\n",
    "\n",
    "# Step 3: Compute scaled dot-product attention\n",
    "attention_scores = torch.matmul(query, key.transpose(-2, -1)) / (head_dim ** 0.5)\n",
    "attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "context = torch.matmul(attention_weights, value)\n",
    "\n",
    "# Step 4: Concatenate and project back\n",
    "context = context.view(1, -1, embed_dim)\n",
    "out_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "out_proj.weight = nn.Parameter(o)\n",
    "output = out_proj(context)\n",
    "\n",
    "# Print the shape of the output tensor\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae3607a-fd1a-4b65-ae1e-96d9d4d37d87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4cbec97a-fad6-4fd4-9579-56bf331fd389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_scores tensor([[[-0.0198]]])\n",
      "attention_weights tensor([[[1.]]])\n",
      "context tensor([[[-0.1662, -0.0868, -0.0580]]])\n",
      "tensor([[ 0.0391,  0.0267, -0.0697]])\n"
     ]
    }
   ],
   "source": [
    "# remove bs\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the input tensor (column vectors)\n",
    "x = torch.tensor([[[-0.1, 0.1, 0.3]]])\n",
    "x = x.reshape(1, 3)\n",
    "\n",
    "q = torch.tensor(  [[-0.3561,  0.3674, -0.5108],\n",
    "                    [ 0.5146, -0.4764, -0.1490],\n",
    "                    [ 0.5072, -0.2932, -0.5633]]).float()\n",
    "k = torch.tensor(  [[-0.4932, -0.4468,  0.0736],\n",
    "                    [-0.6879, -0.4689, -0.1026],\n",
    "                    [ 0.1847,  0.1858,  0.4469]]).float()\n",
    "v = torch.tensor(  [[-0.4110, -0.4083, -0.5549],\n",
    "                    [ 0.3921, -0.0746, -0.1336],\n",
    "                    [-0.6555, -0.3418, -0.2980]]).float()\n",
    "o = torch.tensor([[-0.3601,  0.2771, -0.0573],\n",
    "                  [-0.0896,  0.0567, -0.2882],\n",
    "                  [ 0.3200,  0.1517,  0.0580]]).float()\n",
    "\n",
    "# Define the model parameters\n",
    "embed_dim = 3\n",
    "num_heads = 1\n",
    "head_dim = embed_dim // num_heads\n",
    "\n",
    "# Step 1: Linear projections for queries, keys, and values\n",
    "query = x@q.T\n",
    "key = x@k.T\n",
    "value = x@v.T\n",
    "\n",
    "# Reshape query, key, and value to have shape (batch_size, num_heads, seq_len, head_dim)\n",
    "query = query.view(num_heads, -1, head_dim)\n",
    "key = key.view(num_heads, -1, head_dim)\n",
    "value = value.view(num_heads, -1, head_dim)\n",
    "\n",
    "# Step 3: Compute scaled dot-product attention\n",
    "attention_scores = torch.matmul(query, key.transpose(-2, -1)) / (head_dim ** 0.5)\n",
    "attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "context = torch.matmul(attention_weights, value)\n",
    "\n",
    "\n",
    "print('attention_scores', attention_scores)\n",
    "print('attention_weights', attention_weights)\n",
    "print('context', context)\n",
    "\n",
    "\n",
    "# Step 4: Concatenate and project back\n",
    "context = context.view(-1, embed_dim)\n",
    "output = context@o.T\n",
    "\n",
    "# Print the shape of the output tensor\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3028a337-ebc4-4801-bd99-3c90c5fc5a25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fcb30174-5ee3-4bbb-a8f6-e908b830d308",
   "metadata": {},
   "source": [
    "## Example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "58ab7852-0cc5-48ee-9050-d3a64fe609a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1469, -0.1176,  0.2046],\n",
      "         [-0.1481, -0.1185,  0.2045]]], grad_fn=<TransposeBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the input tensor\n",
    "#x = torch.randn(1, 2, 3)\n",
    "x = torch.tensor([[[-0.1767, -0.2996, -0.6140],\n",
    "                   [ 0.4852, -1.1095, -0.3858]]])\n",
    "\n",
    "# Create the multi-head attention layer\n",
    "layer = nn.MultiheadAttention(embed_dim=3, num_heads=1, bias=False, batch_first=True)\n",
    "\n",
    "custom_weights = torch.tensor( [[-0.3561,  0.3674, -0.5108],\n",
    "                                [ 0.5146, -0.4764, -0.1490],\n",
    "                                [ 0.5072, -0.2932, -0.5633],\n",
    "                                [-0.4932, -0.4468,  0.0736],\n",
    "                                [-0.6879, -0.4689, -0.1026],\n",
    "                                [ 0.1847,  0.1858,  0.4469],\n",
    "                                [-0.4110, -0.4083, -0.5549],\n",
    "                                [ 0.3921, -0.0746, -0.1336],\n",
    "                                [-0.6555, -0.3418, -0.2980]]).float()\n",
    "layer.in_proj_weight = nn.Parameter(custom_weights)\n",
    "\n",
    "custom_out_proj = torch.tensor([[-0.3601,  0.2771, -0.0573],\n",
    "                                [-0.0896,  0.0567, -0.2882],\n",
    "                                [ 0.3200,  0.1517,  0.0580]]).float()\n",
    "layer.out_proj.weight = nn.Parameter(custom_out_proj)\n",
    "\n",
    "# Perform the forward pass\n",
    "# You can use x for both queries, keys, and values in this example\n",
    "output_tensor, attn_output_weights = layer(x, x, x)  \n",
    "\n",
    "# Print the shape of the output tensor\n",
    "print(output_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a83dc6-0238-49a5-a90a-88dcee2af372",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f76bde34-4de3-4b05-b6c4-a46716fc5f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 3])\n",
      "tensor([[[-0.1469, -0.1176,  0.2046],\n",
      "         [-0.1481, -0.1185,  0.2045]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the input tensor\n",
    "x = torch.tensor([[[-0.1767, -0.2996, -0.6140],\n",
    "                   [ 0.4852, -1.1095, -0.3858]]])\n",
    "\n",
    "q = torch.tensor(  [[-0.3561,  0.3674, -0.5108],\n",
    "                    [ 0.5146, -0.4764, -0.1490],\n",
    "                    [ 0.5072, -0.2932, -0.5633]]).float()\n",
    "k = torch.tensor(  [[-0.4932, -0.4468,  0.0736],\n",
    "                    [-0.6879, -0.4689, -0.1026],\n",
    "                    [ 0.1847,  0.1858,  0.4469]]).float()\n",
    "v = torch.tensor(  [[-0.4110, -0.4083, -0.5549],\n",
    "                    [ 0.3921, -0.0746, -0.1336],\n",
    "                    [-0.6555, -0.3418, -0.2980]]).float()\n",
    "o = torch.tensor([[-0.3601,  0.2771, -0.0573],\n",
    "                  [-0.0896,  0.0567, -0.2882],\n",
    "                  [ 0.3200,  0.1517,  0.0580]]).float()\n",
    "\n",
    "# Define the model parameters\n",
    "embed_dim = 3\n",
    "num_heads = 1\n",
    "head_dim = embed_dim // num_heads\n",
    "\n",
    "# Step 1: Linear projections for queries, keys, and values\n",
    "query_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "key_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "value_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "\n",
    "# Custom weights for linear projections\n",
    "query_proj.weight = nn.Parameter(q)\n",
    "key_proj.weight = nn.Parameter(k)\n",
    "value_proj.weight = nn.Parameter(v)\n",
    "\n",
    "# Step 2: Split the input into multiple heads\n",
    "query = query_proj(x)\n",
    "key = key_proj(x)\n",
    "value = value_proj(x)\n",
    "\n",
    "# Reshape query, key, and value to have shape (batch_size, num_heads, seq_len, head_dim)\n",
    "query = query.view(1, num_heads, -1, head_dim)\n",
    "key = key.view(1, num_heads, -1, head_dim)\n",
    "value = value.view(1, num_heads, -1, head_dim)\n",
    "\n",
    "# Step 3: Compute scaled dot-product attention\n",
    "attention_scores = torch.matmul(query, key.transpose(-2, -1)) / (head_dim ** 0.5)\n",
    "attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "context = torch.matmul(attention_weights, value)\n",
    "\n",
    "# Step 4: Concatenate and project back\n",
    "context = context.view(1, -1, embed_dim)\n",
    "out_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "out_proj.weight = nn.Parameter(o)\n",
    "output = out_proj(context)\n",
    "\n",
    "# Print the shape of the output tensor\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede07953-b37e-47ee-a04e-b0b00cbadd68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4940c7f0-3798-496c-a808-077e532a6b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention_scores tensor([[[-0.0198,  0.0028],\n",
      "         [ 0.0438, -0.0465]]])\n",
      "attention_weights tensor([[[0.4944, 0.5056],\n",
      "         [0.5225, 0.4775]]])\n",
      "context tensor([[[0.1460, 0.0982, 0.0741],\n",
      "         [0.1286, 0.0879, 0.0667]]])\n",
      "tensor([[-0.0296, -0.0289,  0.0659],\n",
      "        [-0.0258, -0.0258,  0.0583]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the input tensor\n",
    "x = torch.tensor([[[-0.1, 0.1,  0.3],\n",
    "                   [ 0.4, -1.1, -0.3]]])\n",
    "x = x.reshape(2, 3)\n",
    "\n",
    "q = torch.tensor(  [[-0.3561,  0.3674, -0.5108],\n",
    "                    [ 0.5146, -0.4764, -0.1490],\n",
    "                    [ 0.5072, -0.2932, -0.5633]]).float()\n",
    "k = torch.tensor(  [[-0.4932, -0.4468,  0.0736],\n",
    "                    [-0.6879, -0.4689, -0.1026],\n",
    "                    [ 0.1847,  0.1858,  0.4469]]).float()\n",
    "v = torch.tensor(  [[-0.4110, -0.4083, -0.5549],\n",
    "                    [ 0.3921, -0.0746, -0.1336],\n",
    "                    [-0.6555, -0.3418, -0.2980]]).float()\n",
    "o = torch.tensor([[-0.3601,  0.2771, -0.0573],\n",
    "                  [-0.0896,  0.0567, -0.2882],\n",
    "                  [ 0.3200,  0.1517,  0.0580]]).float()\n",
    "\n",
    "# Define the model parameters\n",
    "embed_dim = 3\n",
    "num_heads = 1\n",
    "head_dim = embed_dim // num_heads\n",
    "\n",
    "# Step 1: Linear projections for queries, keys, and values\n",
    "query = x@q.T\n",
    "key = x@k.T\n",
    "value = x@v.T\n",
    "\n",
    "# Reshape query, key, and value to have shape (batch_size, num_heads, seq_len, head_dim)\n",
    "query = query.view(num_heads, -1, head_dim)\n",
    "key = key.view(num_heads, -1, head_dim)\n",
    "value = value.view(num_heads, -1, head_dim)\n",
    "\n",
    "# Step 3: Compute scaled dot-product attention\n",
    "attention_scores = torch.matmul(query, key.transpose(-2, -1)) / (head_dim ** 0.5)\n",
    "attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "context = torch.matmul(attention_weights, value)\n",
    "\n",
    "\n",
    "print('attention_scores', attention_scores)\n",
    "print('attention_weights', attention_weights)\n",
    "print('context', context)\n",
    "\n",
    "\n",
    "# Step 4: Concatenate and project back\n",
    "context = context.view(-1, embed_dim)\n",
    "output = context@o.T\n",
    "\n",
    "# Print the shape of the output tensor\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c75211-2a09-4597-8bef-7dc671e4962f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
