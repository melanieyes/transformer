{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58528456-8f70-4063-9fcb-ed46b6cef883",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6e30034-e24c-4596-bffb-333bbd8cd753",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(embed_dim=embed_dim, \n",
    "                                          num_heads=num_heads)\n",
    "        self.ffn = nn.Linear(in_features=embed_dim, \n",
    "                             out_features=ff_dim)\n",
    "        self.layernorm_1 = nn.LayerNorm(normalized_shape=embed_dim)\n",
    "        self.layernorm_2 = nn.LayerNorm(normalized_shape=embed_dim)\n",
    "\n",
    "    def forward(self, query, key, value):\n",
    "        attn_output, _ = self.attn(query, key, value)\n",
    "        out_1 = self.layernorm_1(query + attn_output)\n",
    "        ffn_output = self.ffn(out_1)\n",
    "        out_2 = self.layernorm_2(out_1 + ffn_output)\n",
    "        return out_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364279e8-4438-41cb-ac16-e8cd22b7d445",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "909068e7-8b0c-40f1-b8b9-0df8c8d4949f",
   "metadata": {},
   "source": [
    "## Example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61cd23c0-9460-4ea1-955d-767384cd567f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: tensor([[[-0.5958, -0.8128,  1.4086],\n",
      "         [ 1.3946, -0.9003, -0.4943]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the input tensor\n",
    "x = torch.tensor([[[-0.1, 0.1,  0.3],\n",
    "                   [ 0.4, -1.1, -0.3]]])\n",
    "\n",
    "embed_dim = 3\n",
    "ff_dim = 3\n",
    "\n",
    "# MultiheadAttention\n",
    "attn = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=1, bias=False, batch_first=True)\n",
    "\n",
    "custom_weights = torch.tensor( [[-0.3561,  0.3674, -0.5108],\n",
    "                                [ 0.5146, -0.4764, -0.1490],\n",
    "                                [ 0.5072, -0.2932, -0.5633],\n",
    "                                [-0.4932, -0.4468,  0.0736],\n",
    "                                [-0.6879, -0.4689, -0.1026],\n",
    "                                [ 0.1847,  0.1858,  0.4469],\n",
    "                                [-0.4110, -0.4083, -0.5549],\n",
    "                                [ 0.3921, -0.0746, -0.1336],\n",
    "                                [-0.6555, -0.3418, -0.2980]]).float()\n",
    "attn.in_proj_weight = nn.Parameter(custom_weights)\n",
    "\n",
    "custom_out_proj = torch.tensor([[-0.3601,  0.2771, -0.0573],\n",
    "                                [-0.0896,  0.0567, -0.2882],\n",
    "                                [ 0.3200,  0.1517,  0.0580]]).float()\n",
    "attn.out_proj.weight = nn.Parameter(custom_out_proj)\n",
    "\n",
    "# MLP\n",
    "ffn = nn.Linear(in_features=embed_dim, out_features=ff_dim, bias=False)\n",
    "\n",
    "ffn_weight = torch.tensor([[ 0.1580, -0.4134,  0.5055],\n",
    "                            [ 0.3910,  0.5469, -0.0767],\n",
    "                            [-0.3405,  0.4931, -0.4169]]).float()\n",
    "ffn.weight = nn.Parameter(ffn_weight)\n",
    "\n",
    "# LayerNorm\n",
    "layernorm_1 = nn.LayerNorm(normalized_shape=3)\n",
    "layernorm_2 = nn.LayerNorm(normalized_shape=3)\n",
    "\n",
    "# =========== computation ===========\n",
    "query = x\n",
    "key = x\n",
    "value = x\n",
    "\n",
    "attn_output, _ = attn(query, key, value)\n",
    "add1 = query + attn_output\n",
    "out_1 = layernorm_1(add1)\n",
    "ffn_output = ffn(out_1)\n",
    "out_2 = layernorm_2(out_1 + ffn_output)\n",
    "\n",
    "print('output:', out_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a743a90-1d35-4422-8667-40b1a49a3c29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348616aa-d2ce-437c-aa0b-474c0bfafe51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
